# An√°lise de IPTU de Im√≥veis

## üìã Descri√ß√£o

Este projeto implementa um pipeline de dados usando Apache Airflow para processar informa√ß√µes de im√≥veis, incluindo dados de an√∫ncios (CSV) e an√°lises detalhadas (Excel), armazenando tudo em PostgreSQL.

## üöÄ Inicializa√ß√£o

### 1. Configurar Vari√°veis de Ambiente

```bash
echo -e "AIRFLOW_UID=$(id -u)" > .env
```

### 2. Inicializar o Banco de Dados

```bash
docker compose up airflow-init
```

### 3. Iniciar Todos os Servi√ßos

```bash
docker compose up
```

## üåê Acessar a Interface

Ap√≥s o Airflow estar rodando, acesse a interface em: **http://localhost:8080**

### Credenciais de Login:
- **Usu√°rio**: `airflow`
- **Senha**: `airflow`

## üîó Configurar Conex√£o PostgreSQL

Antes do pipeline poder escrever no PostgreSQL, precisamos configurar a conex√£o no Airflow.

### Passos:
1. Na interface do Airflow, v√° em **Admin** ‚Üí **Connections**
2. Clique no bot√£o **"+"** para adicionar uma nova conex√£o
3. Preencha os seguintes detalhes:

```
Connection ID: tutorial_pg_conn
Connection Type: postgres
Host: postgres
Database: airflow
Login: airflow
Password: airflow
Port: 5432
```

4. **Salve** a conex√£o

> üí° **Nota**: Esta configura√ß√£o informa ao Airflow como acessar o banco PostgreSQL rodando no ambiente Docker.

## ‚ñ∂Ô∏è Executar e Explorar o DAG

1. Abra a interface do Airflow e encontre o DAG `imoveis_data_pipeline` na lista
2. Ative o DAG usando o **slider**
3. Execute uma execu√ß√£o usando o **bot√£o play**

Voc√™ pode acompanhar cada tarefa conforme ela executa na **Grid view** e explorar os logs de cada etapa.

### Resultado Esperado:
![Execu√ß√£o do DAG](image/README/1761010528450.png)

Quando executado, o DAG deve retornar algo como:
![Resultado da Execu√ß√£o](image/README/1761010557279.png)

## üóÑÔ∏è Verificar Dados no PostgreSQL

Para visualizar os dados processados, voc√™ pode usar o **DBeaver** (ou qualquer cliente de banco de sua prefer√™ncia).

### Configura√ß√£o no DBeaver:

#### 1. Criar Nova Conex√£o:
- Abra o **DBeaver**
- Clique em **"Nova Conex√£o"** (√≠cone de plug)
- Selecione **"PostgreSQL"**
- Clique em **"Avan√ßar"**

#### 2. Configurar Conex√£o:
```
Servidor:
‚îú‚îÄ‚îÄ Host: localhost
‚îú‚îÄ‚îÄ Porta: 5432
‚îú‚îÄ‚îÄ Database: airflow
‚îú‚îÄ‚îÄ Username: airflow
‚îî‚îÄ‚îÄ Senha: airflow
```

#### 3. Testar Conex√£o:
- Clique em **"Testar Conex√£o"**
- Se aparecer **"Conectado"**, est√° tudo certo!
- Clique em **"OK"** para salvar

## ‚ñ∂Ô∏è Executar Web Scrapping

Ele automatiza um processo de web scraping e enriquecimento de imagens, salvando os resultados em um banco PostgreSQL.
Utiliza Selenium para capturar conte√∫do HTML, Pandas para manipular dados e PIL para pr√©-processamento de imagens.

1. Abra a interface do Airflow e encontre o DAG `crawler_pipeline` na lista.
2. Rode o **Trigger** como foi feito no DAG anterior.
3. Essa tarefa pode demorar um tempo (20 minutos) devido ao scrapping.

Cria 1 tabelas novas no Postgres

Chamada de "anuncios_coletados" a nossa tabela Silver

Utilize o DBeaver para visualizar as novas tabelas, que apresentam metadados extras para o treinamento do modelo.
![1761421425587](image/README/1761421425587.png)

Voc·∫Ω tera uma tabale como essa apos o scrapping!

## Executar a inferencia de IA

**NOTA**: Verifique se voc√™ tem a pasta `data` criada, se n√£o houver, execute `mkdir -p data/`

Vamos utilizar as vLLM Gemini da Google, para isso, vamos configurar a `GEMINI_API_KEY`. 

Entre na UI do Airflow, em Admin -> Variables -> `+` -> Adicione `GEMINI_API_KEY` e a sua chave da Google, apos isso, v√° no pipeline `inferencia_ai_pipeline` e execute.

Voc√™ tera algo como:
![1761421609762](image/README/1761421609762.png)

Apos isso, voc√™ ter√° na pasta `/data` um csv com a classifica√ß√£o gerado pelo Gemini e a sua respectiva pontua√ß√£o calculada.

> Em experiencias anterioes, testeamos o TIMM, um loader de modelos de vis√£o, entretanto o desempenho foi pessimo, como √© possivel ver no kaggle. Com experiencias de projetos CEIA por parte da nossa equipe, optamos pelo uso do Gemini, dado que √© um modelo robusto, uso gratuito(apesar de limitado) e n√£o colocamos penso em cima do nosso pipeline do Airflow. 
> Tentemos utilizar o pytorch antes, mas ele era muito pensado, ent√£o o gemini foi uma boa saida para tirar o peso computacional.


## üìä Estrutura dos Dados

### Tabela `anuncios`:
- `id_imovel`: Identificador √∫nico do im√≥vel
- `id_anuncio`: ID do an√∫ncio
- `url`: URL do an√∫ncio
- `imagens`: URLs das imagens
- `titulo`: T√≠tulo do an√∫ncio
- `descricao`: Descri√ß√£o detalhada

### Tabela `analise`:
- `link`: Link do im√≥vel
- `pontuacao_total`: Pontua√ß√£o total da an√°lise
- `padrao`: Classifica√ß√£o do padr√£o (A, B, C)
- E mais 37 campos de caracter√≠sticas e benfeitorias

### Tabela `anuncios_coletados`:
- `scrapping`: Metadados extraidos pelo scrapping da p√°gina web
- `url`: Url da p√°gina que foi feito o scrapping
- `images`: Imagens preprocessadas antes de serem introduzidas ao modelo

## üõ†Ô∏è Tecnologias Utilizadas

- **Apache Airflow**: Orquestra√ß√£o de pipelines
- **PostgreSQL**: Banco de dados
- **Docker**: Containeriza√ß√£o
- **Pandas**: Processamento de dados
- **Python**: Linguagem de programa√ß√£o

## üìù Comandos √öteis:

```bash
# Parar todos os servi√ßos
docker compose down

# Reiniciar apenas o Airflow
docker compose restart airflow-scheduler

# Ver logs do PostgreSQL
docker compose logs postgres

# Acessar o banco via linha de comando
docker compose exec postgres psql -U airflow -d airflow
```
