# AnÃ¡lise de IPTU de ImÃ³veis

## ğŸ“‹ DescriÃ§Ã£o

Este projeto implementa um pipeline de dados usando Apache Airflow para processar informaÃ§Ãµes de imÃ³veis, incluindo dados de anÃºncios (CSV) e anÃ¡lises detalhadas (Excel), armazenando tudo em PostgreSQL.

## ğŸš€ InicializaÃ§Ã£o

### 1. Configurar VariÃ¡veis de Ambiente

```bash
echo -e "AIRFLOW_UID=$(id -u)" > .env
```

### 2. Inicializar o Banco de Dados

```bash
docker compose up airflow-init
```

### 3. Iniciar Todos os ServiÃ§os

```bash
docker compose up
```

## ğŸŒ Acessar a Interface

ApÃ³s o Airflow estar rodando, acesse a interface em: **http://localhost:8080**

### Credenciais de Login:
- **UsuÃ¡rio**: `airflow`
- **Senha**: `airflow`

## ğŸ”— Configurar ConexÃ£o PostgreSQL

Antes do pipeline poder escrever no PostgreSQL, precisamos configurar a conexÃ£o no Airflow.

### Passos:
1. Na interface do Airflow, vÃ¡ em **Admin** â†’ **Connections**
2. Clique no botÃ£o **"+"** para adicionar uma nova conexÃ£o
3. Preencha os seguintes detalhes:

```
Connection ID: tutorial_pg_conn
Connection Type: postgres
Host: postgres
Database: airflow
Login: airflow
Password: airflow
Port: 5432
```

4. **Salve** a conexÃ£o

> ğŸ’¡ **Nota**: Esta configuraÃ§Ã£o informa ao Airflow como acessar o banco PostgreSQL rodando no ambiente Docker.

## â–¶ï¸ Executar e Explorar o DAG

1. Abra a interface do Airflow e encontre o DAG `imoveis_data_pipeline` na lista
2. Ative o DAG usando o **slider**
3. Execute uma execuÃ§Ã£o usando o **botÃ£o play**

VocÃª pode acompanhar cada tarefa conforme ela executa na **Grid view** e explorar os logs de cada etapa.

### Resultado Esperado:
![ExecuÃ§Ã£o do DAG](image/README/1761010528450.png)

Quando executado, o DAG deve retornar algo como:
![Resultado da ExecuÃ§Ã£o](image/README/1761010557279.png)

## ğŸ—„ï¸ Verificar Dados no PostgreSQL

Para visualizar os dados processados, vocÃª pode usar o **DBeaver** (ou qualquer cliente de banco de sua preferÃªncia).

### ConfiguraÃ§Ã£o no DBeaver:

#### 1. Criar Nova ConexÃ£o:
- Abra o **DBeaver**
- Clique em **"Nova ConexÃ£o"** (Ã­cone de plug)
- Selecione **"PostgreSQL"**
- Clique em **"AvanÃ§ar"**

#### 2. Configurar ConexÃ£o:
```
Servidor:
â”œâ”€â”€ Host: localhost
â”œâ”€â”€ Porta: 5432
â”œâ”€â”€ Database: airflow
â”œâ”€â”€ Username: airflow
â””â”€â”€ Senha: airflow
```

#### 3. Testar ConexÃ£o:
- Clique em **"Testar ConexÃ£o"**
- Se aparecer **"Conectado"**, estÃ¡ tudo certo!
- Clique em **"OK"** para salvar

## ğŸ“Š Estrutura dos Dados

### Tabela `anuncios`:
- `id_imovel`: Identificador Ãºnico do imÃ³vel
- `id_anuncio`: ID do anÃºncio
- `url`: URL do anÃºncio
- `imagens`: URLs das imagens
- `titulo`: TÃ­tulo do anÃºncio
- `descricao`: DescriÃ§Ã£o detalhada

### Tabela `analise`:
- `link`: Link do imÃ³vel
- `pontuacao_total`: PontuaÃ§Ã£o total da anÃ¡lise
- `padrao`: ClassificaÃ§Ã£o do padrÃ£o (A, B, C)
- E mais 37 campos de caracterÃ­sticas e benfeitorias

## ğŸ› ï¸ Tecnologias Utilizadas

- **Apache Airflow**: OrquestraÃ§Ã£o de pipelines
- **PostgreSQL**: Banco de dados
- **Docker**: ContainerizaÃ§Ã£o
- **Pandas**: Processamento de dados
- **Python**: Linguagem de programaÃ§Ã£o

## ğŸ“ Comandos Ãšteis

```bash
# Parar todos os serviÃ§os
docker compose down

# Reiniciar apenas o Airflow
docker compose restart airflow-scheduler

# Ver logs do PostgreSQL
docker compose logs postgres

# Acessar o banco via linha de comando
docker compose exec postgres psql -U airflow -d airflow
```